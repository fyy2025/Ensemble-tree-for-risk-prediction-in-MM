---
title: "SurTree"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(intsurv)
library(survminer)
library(survival)
library(readr)
library(randomForestSRC)
library(ranger)
library(alabama)
library(dplyr)
library(rpart)
library(treeClust)
Sys.unsetenv("RETICULATE_PYTHON")
library(reticulate)
library(rmarkdown)
use_python("/usr/local/bin/python3")
```
```{r}
merged_data=read_csv("./merge\ data\ continuous.csv")
merged_data01=read_csv("./merge\ data\ binary.csv")
```

```{r}
test_survival=load("./Survival2020.RData")
test_expression_level=load("./DESeq2Batch1.RData")
```


```{r}
index_tree <- function(node) {
  # Initialize the index counter
  index <- 0
  
  # Initialize a list to store the nodes by their index
  node_list <- list()
  
  # Initialize a data frame to store the parent-child relationships
  edge_list <- data.frame(parent = integer(), left_child = integer(), right_child = integer(), stringsAsFactors = FALSE)
  
  # Helper function to recursively index nodes and store relationships
  index_node <- function(node) {
    if (is.null(node)) {
      return(NULL)
    }
    
    # Increment the index for the current node
    index <<- index + 1
    
    # Store the current index for this node
    current_index <- index
    
    # Add the node to the list with its index
    node_list[[as.character(current_index)]] <- node
    
    # Initialize left and right child indices as NA
    left_index <- NA
    right_index <- NA
    
    # Process the left child if it exists
    if (!is.null(node$left_child)) {
      left_index <- index_node(node$left_child)
    }
    
    # Process the right child if it exists
    if (!is.null(node$right_child)) {
      right_index <- index_node(node$right_child)
    }
    
    # Add the relationship to the edge list
    edge_list <<- rbind(edge_list, data.frame(parent = current_index, left_child = left_index, right_child = right_index))
    
    # Return the current index
    return(current_index)
  }
  
  # Start indexing from the root node
  index_node(node)
  
  # Return the indexed nodes and relationships
  list(edge_list = edge_list)
}

find_terminal_node <- function(treeinfo,node, observation) {
  # Recursively traverse the tree until a leaf node is reached
  index=1
  while (node$is_branching_node()) {
    # Check the split variable and split value
    feature_index=node$feature
    feature_description=feature[feature_index+1]
    split_str=strsplit(feature_description," ")[[1]]
    split_var <- colnames(train)[as.numeric(split_str[2])]
    split_val <- as.numeric(split_str[length(split_str)])
    
    # Determine the next node based on the split
    if (observation[[split_var]] > split_val) {
      node <- node$left_child
      index=tree_info$left_child[which(tree_info$parent==index)]
    } else {
      node <- node$right_child
      index=tree_info$right_child[which(tree_info$parent==index)]
    }
  }
  
  # Return the index of the leaf node
  return(index)
}
```

```{r}
set.seed(1)
train_index = sample(nrow(merged_data), 400, replace = F)
train = merged_data[train_index,]
test = merged_data[-train_index,]
tau=max(merged_data$time)


train_py <- r_to_py(train)

source_python("./STreeD.py")

model=fit(r_to_py(train))

feature_list=model$binarizer_$continuous_binarizer$column_names_
num_feature=length(feature_list)*model$n_thresholds
feature=c()
for (i in 1:length(feature_list)){
  for (j in 1:length(feature_list[[i]])){
    feature=c(feature,feature_list[[i]][j])
  }
}


tree_info=index_tree(model$tree_)[[1]]


group=rep(0,dim(merged_data)[1])
for (i in 1:length(group)){
  group[i]=find_terminal_node(tree_info,model$tree_,merged_data[i,])
}
table(group)

```

```{r}
streed_initial_loss=rep(0,500)
streed_super_loss=rep(0,500)
forest_loss=rep(0,500)
num_leaves=rep(0,500)

for (seed in 1:500){
  set.seed(seed)
  train_index = sample(nrow(merged_data), 400, replace = F)
  train = merged_data[train_index,]
  test = merged_data[-train_index,]
  tau=max(merged_data$time)
  
  structure=TRUE
  model1=ranger(Surv(time, status) ~ .,data=train,num.tree=500,mtry=ncol(train)-2,splitrule = "logrank", min.node.size = 5)
  
  train_py <- r_to_py(train)
  
  source_python("./STreeD.py")
  
  model=fit(r_to_py(train))
  
  feature_list=model$binarizer_$continuous_binarizer$column_names_
  num_feature=length(feature_list)*model$n_thresholds
  feature=c()
  for (i in 1:length(feature_list)){
    for (j in 1:length(feature_list[[i]])){
      feature=c(feature,feature_list[[i]][j])
    }
  }
  
  
  tree_info=index_tree(model$tree_)[[1]]
  
  
  group=rep(0,dim(train)[1])
  for (i in 1:length(group)){
    group[i]=find_terminal_node(tree_info,model$tree_,train[i,])
  }
  
  terminal_nodes=as.numeric(names(table(group)))
  
  for (i in 1:nrow(train)){
    group[i]=which(terminal_nodes==group[i])
  }
  
  chf_value_dict=list() # a list of lists of chf values
  time_dict=list() # a list of lists of time points
  # We now make sure every leaf has at least 2 observations, otherwise the tree structure might not pertain in 10-fold cross validation. For leaves with 1 or 2 observations, we randomly combine this leaf with another leaf.
  leaf_size=as.vector(table(group))
  single_leaf=as.vector(which(leaf_size<=2))
  normal_leaf=as.vector(which(leaf_size>2))
  
  reassign_dict=as.vector(1:sum(is.na(tree_info$left_child)))
  for (i in 1:length(single_leaf)){
    new_assignment=sample(normal_leaf,1)
    group[which(group==single_leaf[i])]=new_assignment
    reassign_dict[single_leaf[i]]=new_assignment
  }
  
  # change the group from 1 3 4 5 to 1 2 3 4 after combining leaves
  num_leaf=length(table(group))
  loose_group=as.numeric(names(table(group)))
  for (i in 1:length(group)){
    group[i]=which(loose_group==group[i])
  }
  for (i in 1:length(reassign_dict)){
    reassign_dict[i]=which(loose_group==reassign_dict[i])
  }
  
  num_leaf=length(table(group))
  
  # keep the first group allocation for CV
  first_group=group
  
  # save the chf for initial tree on full train data
  chf_list=list()
  time_list=list()
  train_group=cbind(train,group)
  for (i in 1:num_leaf){
    chf_list[[i]]=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_group,group==i))$cumhaz
    time_list[[i]]=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_group,group==i))$time
  }
  
  # 10-fold cross validation
  if (min(table(group))>=2){ # condition automatically fulfilled from the recombining step
    fold=10
    sampleframe = rep(1:fold, ceiling( nrow(train)/fold ) )
    CV_index=sample(sampleframe,nrow(train) ,  replace=FALSE )
    # CV_index=rep(1:10,nrow(train)/10)
    CV_losses=list()
  }else{
    fold=nrow(train)
    CV_index=1:nrow(train)
    CV_losses=list()
  }
  
  # Store the time, event data for calculating test c index
  row_time=c()
  row_event=c()
  risk_score=list()
  for (i in 1:num_leaf){
    risk_score[[i]]=c(0)
  }
  
  
  # Cross Validation
  for (step in 1:fold){
    CV_test=train[CV_index==step,]
    CV_train=train[CV_index!=step,]
    group=first_group[CV_index!=step] # if 10-fold, drop 10 elements in group
    test_group=first_group[CV_index==step]
    if (length(table(group))!=num_leaf){
      structure=FALSE
      break
    }
    
    #For each validation fold, we update the estimator in each leaf using the data from this leaf specifically.
    first_chf=list()
    first_time=list()
    for (i in 1:num_leaf){
      result_combined=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(CV_train,group==i))
      chf=result_combined$cumhaz
      first_chf[[i]]=chf
      time=result_combined$time
      first_time[[i]]=time
    }
    chf_value_dict[[1]]=first_chf
    time_dict[[1]]=first_time
    
  
    train_set=cbind(CV_train,group)
    result=tryCatch( # some time only event or only non-event, cannot do pairwise log rank
      expr={
        pairwise_survdiff(Surv(time, status)~group, data=train_set)
      },
      error = function(e) {
        message("There was an error message.") # prints structure of exception
        return(list(NULL))
      }
    )
    
    if (length(result)==1){
      structure=FALSE
      break
    }
    result=pairwise_survdiff(Surv(time, status)~group, data=train_set)
    
    pvalue=result$p.value
    index=which(pvalue == max(pvalue,na.rm=T), arr.ind = TRUE)
    row=as.integer(rownames(pvalue)[index[1,1]])
    col=as.integer(colnames(pvalue)[index[1,2]]) #row and col are index of the leaves to be combined
    result_combined_2=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_set,group%in%c(row,col)))
    chf_value_combined_2=result_combined_2$cumhaz
    chf_time_combined_2=result_combined_2$time
    dict=as.vector(1:num_leaf) # store the process of combining leaves
    group_dict=list(dict)
    
    # Leaf Combination process
    if (num_leaf>1){
      for (i in 2:(num_leaf)){
        #record the current chf function value and time
        chf=chf_value_dict[[i-1]]
        time=time_dict[[i-1]]
        for (j in 1:num_leaf){
          if (group_dict[[i-1]][j]%in%c(row,col)){
            chf[[j]]=result_combined_2$cumhaz
            time[[j]]=result_combined_2$time
          }
        }
        
        chf_value_dict[[i]]=chf
        time_dict[[i]]=time
        
        #update the group assignment and decide the next combination
        
        group=ifelse(group%in%c(row,col), min(row,col), group)
        #print(paste0("Combine group ",row," and ",col," into group ",min(row,col)))
        
        for (ii in 1:num_leaf){
          if (dict[ii]==max(row,col)){
            dict[ii]=min(row,col)
          }
        }
        group_dict[[i]]=dict
        
        if (i != num_leaf){
          train_2=cbind(CV_train,group)
          result=pairwise_survdiff(Surv(time, status)~group, data=train_2)
          pvalue=result$p.value
          index=which(pvalue == max(pvalue,na.rm=T), arr.ind = TRUE)
          row=as.integer(rownames(pvalue)[index[1,1]])
          col=as.integer(colnames(pvalue)[index[1,2]])
          
          result_combined_2=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_2,group%in%c(row,col)))
        }
      }
    } # end of fitting the trees
    
    # now chf_value_dict is a list with length=num_leaf, each element of the list is still a list with length=num_leaf
    # The jth element of the ith element of chf_value_dict is the chf values for the jth leaf in the tree-like model with num_leaf-i+1 total leaves left
    # Same result for time_dict
  
  
    # store the risk score for test observations calculated by each tree
    
    for (j in 1:nrow(CV_test)){
      row_time=c(row_time,CV_test$time[j])
      row_event=c(row_event,CV_test$status[j])
      for (i in 1:num_leaf){
        value=chf_value_dict[[i]][[test_group[j]]]
        time=time_dict[[i]][[test_group[j]]]
  
        index=length(time) # find the index of the largest time value smaller than t
        while(tau<time[index] && index>1){
          index=index-1
        }
        risk_score[[i]]=c(risk_score[[i]],1-exp(-value[index]))
  
      }
    }
    
  }
  
  # Remove the 0 at the beginning of each risk score
  for (i in 1:num_leaf){
    risk_score[[i]]=risk_score[[i]][-1]
  }
  # Now risk_score is a list with length=num_leaf. The ith element of risk_score is the failure probability of each patient predicted using the tree-like model with num_leaf-i+1 leaves
  
  if (structure==FALSE){
    next
  }
  
  
  # Optimization of ensemble weights
  fn=function(x){
    result=0
    for (i in 1:num_leaf){
      result=result+x[i]*risk_score[[i]]
    }
    fn=cIndex(row_time,row_event,as.vector(result))[1]
    
    fn
  }
  
  heq=function(x){
    h=rep(0,1)
    for (i in 1:num_leaf){
      h[1]=h[1]+x[i]
    }
    h[1]=h[1]-1
    h
  }
  
  hin=function(x){
    h=rep(NA,1)
    for (i in 1:num_leaf){
      h[i]=x[i]
    }
    h
  }
  
  set.seed(1111)
  p0=runif(num_leaf)
  ans=constrOptim.nl(par=p0, fn=fn, heq=heq, hin=hin)
  
  parameter=ans$par/sum(ans$par)
  
  # Use the full group
  
  # predict2=predict(model2, data=train)
  # chf_time=predict2$unique.death.times
  # chf_value=predict2$chf
  # chf_value_dict=list()
  # for (i in 1:length(num_leaf)){
  #   chf_value_dict[[i]]=list(chf_list[[i]]$chf)
  #   time_dict[[i]]=list(chf_list[[i]]$time)
  # }
  # 
  # for (i in 1:nrow(train)) {
  #   for (j in 1:length(leaf)) {
  #     if (all(chf_value[i,] == leaf[[j]])) {
  #       group[i]=reassign_dict[j]
  #     }
  #   }
  # }
  # 
  # chf_value_dict=list(leaf) # a list of lists of chf values
  # time=list()
  # for (i in 1:num_leaf){
  #   time[[i]]=chf_time
  # }
  # time_dict=list(time) # a list of lists of time points
  group=first_group
  chf=list()
  time=list()
  
  chf_value_dict=list(chf_list)
  time_dict=list(time_list)
  
  test_risk_score=list()
  for (i in 1:num_leaf){
    test_risk_score[[i]]=rep(NA,nrow(test))
  }
  
  train_set=cbind(train,group)
  
  # Perform the log-rank test
  result <- pairwise_survdiff(Surv(time, status)~group, data=train_set)
  
  pvalue=result$p.value
  index=which(pvalue == max(pvalue,na.rm=T), arr.ind = TRUE)
  row=as.integer(rownames(pvalue)[index[1,1]])
  col=as.integer(colnames(pvalue)[index[1,2]])
  
  result_combined_2=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_set,group%in%c(row,col)))
  
  chf_value_combined_2=result_combined_2$cumhaz
  chf_time_combined_2=result_combined_2$time
  
  dict=as.vector(1:num_leaf)
  group_dict=list(dict)
  
  if (num_leaf>1){
    for (i in 2:(num_leaf)){
      #record the current chf function value and time
      chf=chf_value_dict[[i-1]]
      time=time_dict[[i-1]]
      for (j in 1:num_leaf){
        if (group_dict[[i-1]][j]%in%c(row,col)){
          chf[[j]]=result_combined_2$cumhaz
          time[[j]]=result_combined_2$time
        }
      }
      
      chf_value_dict[[i]]=chf
      time_dict[[i]]=time
      
      #update the group assignment and decide the next combination
      
      group=ifelse(group%in%c(row,col), min(row,col), group)
      #print(paste0("Combine group ",row," and ",col," into group ",min(row,col)))
      
      for (ii in 1:num_leaf){
        if (dict[ii]==max(row,col)){
          dict[ii]=min(row,col)
        }
      }
      group_dict[[i]]=dict
      
      if (i != num_leaf){
        train_2=cbind(train,group)
        result=pairwise_survdiff(Surv(time, status)~group, data=train_2)
        pvalue=result$p.value
        index=which(pvalue == max(pvalue,na.rm=T), arr.ind = TRUE)
        row=as.integer(rownames(pvalue)[index[1,1]])
        col=as.integer(colnames(pvalue)[index[1,2]])
        
        result_combined_2=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_2,group%in%c(row,col)))
      }
    }
  } # end of fitting the trees
  
  # Now evaluate model performance using the test dataset
  # test_predict=predict(model2, data=test)
  # chf_predict=test_predict$chf
  # test_group=rep(NA,nrow(test))
  
  test_group=rep(0,dim(test)[1])
  for (i in 1:length(test_group)){
    test_group[i]=find_terminal_node(tree_info,model$tree_,test[i,])
  }
  
  for (i in 1:nrow(test)){
    test_group[i]=reassign_dict[which(terminal_nodes==test_group[i])]
  }
  
  # for (i in 1:nrow(test)) {
  #   for (j in 1:length(terminal_nodes)) {
  #     if (all(chf_predict[i,] == leaf[[j]])) {
  #       test_group[i]=reassign_dict[j]
  #     }
  #   }
  # }
  
  for (j in 1:nrow(test)){
    for (i in 1:num_leaf){
      value=chf_value_dict[[i]][[test_group[j]]]
      time=time_dict[[i]][[test_group[j]]]
      
      index=length(time) # find the index of the largest time value smaller than t
      while(tau<time[index] && index>1){
        index=index-1
      }
      test_risk_score[[i]][j]=1-exp(-value[index])
      
    }
    
  }
  
  test_c_index=rep(0,num_leaf)
  for (i in 1:num_leaf){
    test_c_index[i]=cIndex(test$time,test$status,as.vector(test_risk_score[[i]]))[1]
  }
  result=0
  for (i in 1:num_leaf){
    result=result+parameter[i]*test_risk_score[[i]] # calculating the ensemble risk score
  }
  
  streed_super_loss[seed]=cIndex(test$time,test$status,as.vector(result))[1]
  streed_initial_loss[seed]=test_c_index[1]
  
  test_predict=predict(model1, data=test)
  chf_predict=test_predict$chf
  chf_predict_time=test_predict$unique.death.times
  test_risk_score2=rep(0,nrow(test))
  
  for (j in 1:nrow(test)){
    index=length(chf_predict_time) # find the index of the largest time value smaller than t
    while(tau<chf_predict_time[index] && index>1){
      index=index-1
    }
    test_risk_score2[j]=1-exp(-chf_predict[j,index])
  }
  
  forest_loss[seed]=cIndex(test$time,test$status,as.vector(test_risk_score2))[1]
  num_leaves[seed]=num_leaf
  print(seed)
}
```


```{r}
find_terminal_node <- function(treeinfo,node, observation) {
  # Recursively traverse the tree until a leaf node is reached
  index=1
  while (node$is_branching_node()) {
    # Check the split variable and split value
    feature_index=node$feature
  
    split_var <- feature[feature_index+1]
    split_val <- 0.5
    
    # Determine the next node based on the split
    if (observation[[split_var]] < split_val) {
      node <- node$left_child
      index=tree_info$left_child[which(tree_info$parent==index)]
    } else {
      node <- node$right_child
      index=tree_info$right_child[which(tree_info$parent==index)]
    }
  }
  
  # Return the index of the leaf node
  return(index)
}
```

```{r}
streed_super_loss01=rep(0,50)
streed_initial_loss01=rep(0,50)
forest_loss01=rep(0,50)
num_leaves01=rep(0,50)

for (seed in 2:2){
  set.seed(seed)
  train_index = sample(nrow(merged_data), 400, replace = F)
  train = merged_data01[train_index,]
  test = merged_data01[-train_index,]
  tau=max(merged_data$time)
  
  structure=TRUE
  model1=ranger(Surv(time, status) ~ .,data=train,num.tree=500,mtry=ncol(train)-2,splitrule = "logrank", min.node.size = 5)
  
  train_py <- r_to_py(train)
  
  source_python("./STreeD.py")
  
  model=fit(r_to_py(train))
  
  feature=colnames(train)[-c(ncol(train),ncol(train)-1)]
  
  tree_info=index_tree(model$tree_)[[1]]
  
  
  group=rep(0,dim(train)[1])
  for (i in 1:length(group)){
    group[i]=find_terminal_node(tree_info,model$tree_,train[i,])
  }
  
  terminal_nodes=as.numeric(names(table(group)))
  
  for (i in 1:nrow(train)){
    group[i]=which(terminal_nodes==group[i])
  }
  
  chf_value_dict=list() # a list of lists of chf values
  time_dict=list() # a list of lists of time points
  # We now make sure every leaf has at least 2 observations, otherwise the tree structure might not pertain in 10-fold cross validation. For leaves with 1 or 2 observations, we randomly combine this leaf with another leaf.
  leaf_size=as.vector(table(group))
  single_leaf=as.vector(which(leaf_size<=2))
  normal_leaf=as.vector(which(leaf_size>2))
  
  reassign_dict=as.vector(1:sum(is.na(tree_info$left_child)))
  for (i in 1:length(single_leaf)){
    new_assignment=sample(normal_leaf,1)
    group[which(group==single_leaf[i])]=new_assignment
    reassign_dict[single_leaf[i]]=new_assignment
  }
  
  # change the group from 1 3 4 5 to 1 2 3 4 after combining leaves
  num_leaf=length(table(group))
  loose_group=as.numeric(names(table(group)))
  for (i in 1:length(group)){
    group[i]=which(loose_group==group[i])
  }
  for (i in 1:length(reassign_dict)){
    reassign_dict[i]=which(loose_group==reassign_dict[i])
  }
  
  num_leaf=length(table(group))
  
  # keep the first group allocation for CV
  first_group=group
  
  # save the chf for initial tree on full train data
  chf_list=list()
  time_list=list()
  train_group=cbind(train,group)
  for (i in 1:num_leaf){
    chf_list[[i]]=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_group,group==i))$cumhaz
    time_list[[i]]=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_group,group==i))$time
  }
  
  # 10-fold cross validation
  if (min(table(group))>=2){ # condition automatically fulfilled from the recombining step
    fold=10
    sampleframe = rep(1:fold, ceiling( nrow(train)/fold ) )
    CV_index=sample(sampleframe,nrow(train) ,  replace=FALSE )
    # CV_index=rep(1:10,nrow(train)/10)
    CV_losses=list()
  }else{
    fold=nrow(train)
    CV_index=1:nrow(train)
    CV_losses=list()
  }
  
  # Store the time, event data for calculating test c index
  row_time=c()
  row_event=c()
  risk_score=list()
  for (i in 1:num_leaf){
    risk_score[[i]]=c(0)
  }
  
  
  # Cross Validation
  for (step in 1:fold){
    CV_test=train[CV_index==step,]
    CV_train=train[CV_index!=step,]
    group=first_group[CV_index!=step] # if 10-fold, drop 10 elements in group
    test_group=first_group[CV_index==step]
    if (length(table(group))!=num_leaf){
      structure=FALSE
      break
    }
    
    #For each validation fold, we update the estimator in each leaf using the data from this leaf specifically.
    first_chf=list()
    first_time=list()
    for (i in 1:num_leaf){
      result_combined=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(CV_train,group==i))
      chf=result_combined$cumhaz
      first_chf[[i]]=chf
      time=result_combined$time
      first_time[[i]]=time
    }
    chf_value_dict[[1]]=first_chf
    time_dict[[1]]=first_time
    
  
    train_set=cbind(CV_train,group)
    result=tryCatch( # some time only event or only non-event, cannot do pairwise log rank
      expr={
        pairwise_survdiff(Surv(time, status)~group, data=train_set)
      },
      error = function(e) {
        message("There was an error message.") # prints structure of exception
        return(list(NULL))
      }
    )
    
    if (length(result)==1){
      structure=FALSE
      break
    }
    result=pairwise_survdiff(Surv(time, status)~group, data=train_set)
    
    pvalue=result$p.value
    index=which(pvalue == max(pvalue,na.rm=T), arr.ind = TRUE)
    row=as.integer(rownames(pvalue)[index[1,1]])
    col=as.integer(colnames(pvalue)[index[1,2]]) #row and col are index of the leaves to be combined
    result_combined_2=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_set,group%in%c(row,col)))
    chf_value_combined_2=result_combined_2$cumhaz
    chf_time_combined_2=result_combined_2$time
    dict=as.vector(1:num_leaf) # store the process of combining leaves
    group_dict=list(dict)
    
    # Leaf Combination process
    if (num_leaf>1){
      for (i in 2:(num_leaf)){
        #record the current chf function value and time
        chf=chf_value_dict[[i-1]]
        time=time_dict[[i-1]]
        for (j in 1:num_leaf){
          if (group_dict[[i-1]][j]%in%c(row,col)){
            chf[[j]]=result_combined_2$cumhaz
            time[[j]]=result_combined_2$time
          }
        }
        
        chf_value_dict[[i]]=chf
        time_dict[[i]]=time
        
        #update the group assignment and decide the next combination
        
        group=ifelse(group%in%c(row,col), min(row,col), group)
        #print(paste0("Combine group ",row," and ",col," into group ",min(row,col)))
        
        for (ii in 1:num_leaf){
          if (dict[ii]==max(row,col)){
            dict[ii]=min(row,col)
          }
        }
        group_dict[[i]]=dict
        
        if (i != num_leaf){
          train_2=cbind(CV_train,group)
          result=pairwise_survdiff(Surv(time, status)~group, data=train_2)
          pvalue=result$p.value
          index=which(pvalue == max(pvalue,na.rm=T), arr.ind = TRUE)
          row=as.integer(rownames(pvalue)[index[1,1]])
          col=as.integer(colnames(pvalue)[index[1,2]])
          
          result_combined_2=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_2,group%in%c(row,col)))
        }
      }
    } # end of fitting the trees
    
    # now chf_value_dict is a list with length=num_leaf, each element of the list is still a list with length=num_leaf
    # The jth element of the ith element of chf_value_dict is the chf values for the jth leaf in the tree-like model with num_leaf-i+1 total leaves left
    # Same result for time_dict
  
  
    # store the risk score for test observations calculated by each tree
    
    for (j in 1:nrow(CV_test)){
      row_time=c(row_time,CV_test$time[j])
      row_event=c(row_event,CV_test$status[j])
      for (i in 1:num_leaf){
        value=chf_value_dict[[i]][[test_group[j]]]
        time=time_dict[[i]][[test_group[j]]]
  
        index=length(time) # find the index of the largest time value smaller than t
        while(tau<time[index] && index>1){
          index=index-1
        }
        risk_score[[i]]=c(risk_score[[i]],1-exp(-value[index]))
  
      }
    }
    
  }
  
  # Remove the 0 at the beginning of each risk score
  for (i in 1:num_leaf){
    risk_score[[i]]=risk_score[[i]][-1]
  }
  # Now risk_score is a list with length=num_leaf. The ith element of risk_score is the failure probability of each patient predicted using the tree-like model with num_leaf-i+1 leaves
  
  if (structure==FALSE){
    next
  }
  
  
  # Optimization of ensemble weights
  fn=function(x){
    result=0
    for (i in 1:num_leaf){
      result=result+x[i]*risk_score[[i]]
    }
    fn=cIndex(row_time,row_event,as.vector(result))[1]
    
    fn
  }
  
  heq=function(x){
    h=rep(0,1)
    for (i in 1:num_leaf){
      h[1]=h[1]+x[i]
    }
    h[1]=h[1]-1
    h
  }
  
  hin=function(x){
    h=rep(NA,1)
    for (i in 1:num_leaf){
      h[i]=x[i]
    }
    h
  }
  
  set.seed(1111)
  p0=runif(num_leaf)
  ans=constrOptim.nl(par=p0, fn=fn, heq=heq, hin=hin)
  
  parameter=ans$par/sum(ans$par)
  
  # Use the full group
  
  # predict2=predict(model2, data=train)
  # chf_time=predict2$unique.death.times
  # chf_value=predict2$chf
  # chf_value_dict=list()
  # for (i in 1:length(num_leaf)){
  #   chf_value_dict[[i]]=list(chf_list[[i]]$chf)
  #   time_dict[[i]]=list(chf_list[[i]]$time)
  # }
  # 
  # for (i in 1:nrow(train)) {
  #   for (j in 1:length(leaf)) {
  #     if (all(chf_value[i,] == leaf[[j]])) {
  #       group[i]=reassign_dict[j]
  #     }
  #   }
  # }
  # 
  # chf_value_dict=list(leaf) # a list of lists of chf values
  # time=list()
  # for (i in 1:num_leaf){
  #   time[[i]]=chf_time
  # }
  # time_dict=list(time) # a list of lists of time points
  group=first_group
  chf=list()
  time=list()
  
  chf_value_dict=list(chf_list)
  time_dict=list(time_list)
  
  test_risk_score=list()
  for (i in 1:num_leaf){
    test_risk_score[[i]]=rep(NA,nrow(test))
  }
  
  train_set=cbind(train,group)
  
  # Perform the log-rank test
  result <- pairwise_survdiff(Surv(time, status)~group, data=train_set)
  
  pvalue=result$p.value
  index=which(pvalue == max(pvalue,na.rm=T), arr.ind = TRUE)
  row=as.integer(rownames(pvalue)[index[1,1]])
  col=as.integer(colnames(pvalue)[index[1,2]])
  
  result_combined_2=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_set,group%in%c(row,col)))
  
  chf_value_combined_2=result_combined_2$cumhaz
  chf_time_combined_2=result_combined_2$time
  
  dict=as.vector(1:num_leaf)
  group_dict=list(dict)
  
  if (num_leaf>1){
    for (i in 2:(num_leaf)){
      #record the current chf function value and time
      chf=chf_value_dict[[i-1]]
      time=time_dict[[i-1]]
      for (j in 1:num_leaf){
        if (group_dict[[i-1]][j]%in%c(row,col)){
          chf[[j]]=result_combined_2$cumhaz
          time[[j]]=result_combined_2$time
        }
      }
      
      chf_value_dict[[i]]=chf
      time_dict[[i]]=time
      
      #update the group assignment and decide the next combination
      
      group=ifelse(group%in%c(row,col), min(row,col), group)
      #print(paste0("Combine group ",row," and ",col," into group ",min(row,col)))
      
      for (ii in 1:num_leaf){
        if (dict[ii]==max(row,col)){
          dict[ii]=min(row,col)
        }
      }
      group_dict[[i]]=dict
      
      if (i != num_leaf){
        train_2=cbind(train,group)
        result=pairwise_survdiff(Surv(time, status)~group, data=train_2)
        pvalue=result$p.value
        index=which(pvalue == max(pvalue,na.rm=T), arr.ind = TRUE)
        row=as.integer(rownames(pvalue)[index[1,1]])
        col=as.integer(colnames(pvalue)[index[1,2]])
        
        result_combined_2=survfit(Surv(time,status)~1, type = "fleming-harrington",data=subset(train_2,group%in%c(row,col)))
      }
    }
  } # end of fitting the trees
  
  # Now evaluate model performance using the test dataset
  # test_predict=predict(model2, data=test)
  # chf_predict=test_predict$chf
  # test_group=rep(NA,nrow(test))
  
  test_group=rep(0,dim(test)[1])
  for (i in 1:length(test_group)){
    test_group[i]=find_terminal_node(tree_info,model$tree_,test[i,])
  }
  
  for (i in 1:nrow(test)){
    test_group[i]=reassign_dict[which(terminal_nodes==test_group[i])]
  }
  
  # for (i in 1:nrow(test)) {
  #   for (j in 1:length(terminal_nodes)) {
  #     if (all(chf_predict[i,] == leaf[[j]])) {
  #       test_group[i]=reassign_dict[j]
  #     }
  #   }
  # }
  
  for (j in 1:nrow(test)){
    for (i in 1:num_leaf){
      value=chf_value_dict[[i]][[test_group[j]]]
      time=time_dict[[i]][[test_group[j]]]
      
      index=length(time) # find the index of the largest time value smaller than t
      while(tau<time[index] && index>1){
        index=index-1
      }
      test_risk_score[[i]][j]=1-exp(-value[index])
      
    }
    
  }
  
  test_c_index=rep(0,num_leaf)
  for (i in 1:num_leaf){
    test_c_index[i]=cIndex(test$time,test$status,as.vector(test_risk_score[[i]]))[1]
  }
  result=0
  for (i in 1:num_leaf){
    result=result+parameter[i]*test_risk_score[[i]] # calculating the ensemble risk score
  }
  
  streed_super_loss01[seed]=cIndex(test$time,test$status,as.vector(result))[1]
  streed_initial_loss01[seed]=test_c_index[1]
  
  test_predict=predict(model1, data=test)
  chf_predict=test_predict$chf
  chf_predict_time=test_predict$unique.death.times
  test_risk_score2=rep(0,nrow(test))
  
  for (j in 1:nrow(test)){
    index=length(chf_predict_time) # find the index of the largest time value smaller than t
    while(tau<chf_predict_time[index] && index>1){
      index=index-1
    }
    test_risk_score2[j]=1-exp(-chf_predict[j,index])
  }
  
  forest_loss01[seed]=cIndex(test$time,test$status,as.vector(test_risk_score2))[1]
  num_leaves01[seed]=num_leaf
  print(seed)
}
```


```{r}
summary2=read.csv("./c\ index\ comparison.csv")
summary=read.csv("./STreeD\ c\ index\ comparison.csv")
prune_summary=read.csv("./prune_summary.csv")
# summary2$streed_initial_loss=streed_initial_loss
# summary2$streed_super_loss=streed_super_loss
# summary2$streed_initial_loss01=streed_initial_loss01
# summary2$streed_super_loss01=streed_super_loss01
# 
# summary2$streed_super_ratio=summary2$streed_super_loss/summary2$initial_loss
# summary2$streed_initial_ratio=summary2$streed_initial_loss/summary2$initial_loss
par(cex.axis=0.8, mar=c(7, 10, 5, 1))
boxplot(cbind(summary[3:6],summary2,prune_summary[1:4]),beside=T,horizontal=T,las=1,xlab="C index")
```


```{r}
overall_summary=cbind(summary[3:6],summary2,prune_summary[1:4])
streed_initial_ratio=overall_summary$streed_initial_loss/overall_summary$initial_loss
streed_super_ratio=overall_summary$streed_super_loss/overall_summary$initial_loss
boxplot(cbind(streed_initial_ratio,streed_super_ratio),beside=T,horizontal=T,las=1,xlab="C index")
```

```{r}
par(cex.axis=0.8, mar=c(9, 10, 5, 1))
summary=as.data.frame(cbind(streed_initial_loss,streed_initial_loss01,streed_super_loss,streed_super_loss01,forest_loss,forest_loss01))
# write.csv(summary,"Pruned STreeD.csv",row.names = F)
boxplot(summary,beside=T,horizontal=T,las=1,xlab="C index")
summary$initial01_ratio=summary$streed_initial_loss01/summary$streed_initial_loss
summary$super01_ratio=summary$streed_super_loss01/summary$streed_initial_loss
summary$forest01_ratio=summary$forest_loss01/summary$streed_initial_loss
summary$super_ratio=summary$streed_super_loss/summary$streed_initial_loss
summary$forest_ratio=summary$forest_loss/summary$streed_initial_loss
summary_ratio=summary[,7:11]
par(cex.axis=0.8, mar=c(8, 9, 5, 1))
boxplot(summary,beside=T,horizontal=T,las=1,xlab="C index")
boxplot(summary2[1:10],beside=T,horizontal=T,las=1,xlab="C index")
```

